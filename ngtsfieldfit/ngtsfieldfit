#!/usr/bin/env python

from astropy.io import fits 
import numpy as np 
import os, sys
import os.path
import time
from tqdm import tqdm 
from multiprocessing import Pool
import multiprocessing

def write_lightcurves(i):
    # Firs tchange the working directory
    os.chdir(home + '/{:}/{:}'.format(prod_id, obj_ids[i]) )

    # Get the lightcurve data
    time, mag, mag_err = lightcurves.data[i]

    # Convert time to HJD from 
    time = time/ 86400. + 6658.5

    # Now save lightcurve
    np.savetxt('ngts.lc'.format(prod_id, obj_ids[i]), np.array([time.tolist(), mag.tolist(), mag_err.tolist(), mag.tolist(), mag_err.tolist()]).T )
    
    # Now add to writing lightcurve pool
    
def bin_lightcurves(i):
    # First, cd to the directory
    os.chdir(home + '/{:}/{:}'.format(prod_id, obj_ids[i]) )  

    # The wait for the file to be made by the "write_lightcurves" processes...
    while not os.path.exists('ngts.lc'):
        time.sleep(1)
    os.system('ngtsbin ngts.lc 2')
    os.system('ngtsbin ngts.lc 30')


def fit_lightcurve(i):
    # Get the object ID
    obj_id = info.data[i]["OBJ_ID"]

    # Change working directory to it
    os.chdir(home + '/{:}/{:}'.format(prod_id, obj_id) )

    # Get transit parameters
    period = info.data[i]["PERIOD"] / 86400.           # Period in days                '
    epoch = info.data[i]["EPOCH"] / 86400. + 6658.5    # epoch in HJD-2450000 / days
    width = info.data[i]["WIDTH"] /  86400.            # Width in days
    depth = info.data[i]["DEPTH"]* -1.                 # depth in mmag
    radius_1 = np.pi*width/period
    k = np.sqrt(depth)
    PEAK =info.data[i]["RANK"]


    # Make the fitting call 
    os.system('ngtsfit --filename 30_min_lc.dat --t_zero {:.6f} --period {:.6f} --radius_1 {:.3f} --k {:.3f} --gpu'.format(epoch,period, radius_1, k))

    # Now make the plotting call, pipeing it to ngtslog and background since
    # it wont be as long as the fit (hopefully).
    os.system('ngtscorner output.dat 2_min_lc.dat 30_min_lc.dat > ngtsfit.log')

    # Now rename files based on index
    os.system('mv ngtsfit_best_model.png ngtsfit_best_model_{:}.png'.format(PEAK))
    os.system('mv ngtsfit_corner_calculated_parameters.png ngtsfit_corner_calculated_parameters_{:}.png'.format(PEAK))
    os.system('mv ngtsfit_corner_fitted_parameters.png ngtsfit_corner_fitted_parameters{:}.png'.format(PEAK))
    os.system('mv ngtsfit.log ngtsfit_{:}.log'.format(PEAK))
    
    # Now remove the output.dat to keep filespace at a minimum
    os.system('rm output.dat')

if __name__ == '__main__':

    # Load the fits file
    h = fits.open(sys.argv[1])

    # Now define a couple of paths 
    prod_id = str(h[0].header['PROD_ID'])
    print('Processing {:} [product ID {:}]'.format(h[0].header['FIELD'] +'_' +  h[0].header['TAG'], prod_id))

    # Now create the product ID folder
    # Run this on single thread since it's quick
    os.system('rm -r -f {:}'.format(prod_id))
    os.system('mkdir -p {:}'.format(prod_id))

    # Now we need to get a list of independent object IDs
    obj_ids = np.unique(h[4].data["OBJ_ID"])  
    print('\tCreating file structure... ', end=''); sys.stdout.flush()
    for i in range(len(obj_ids))[:] : os.system('mkdir {:}/{:}'.format(prod_id, obj_ids[i]))
    print('done.')

    # Create  this to allow multiprocessing
    home = os.getcwd()
    info = h[4]
    lightcurves = h[5]


    print('\tCreating lightcurve files... ', end=''); sys.stdout.flush()
    writing_pool = Pool(4)
    binning_pool = Pool(8)
 
    writing_pool.map_async(write_lightcurves, range(len(obj_ids))[:])
    binning_pool.map_async(bin_lightcurves, range(len(obj_ids))[:])

    writing_pool.close()
    writing_pool.join()
    binning_pool.close()
    binning_pool.join()


    os.chdir(home)
    print(' ... done.')

    # Now we need to cycle all the indexes
    for i in range(len(h[4].data))[:]:
        fit_lightcurve(i)
    os.chdir(home)
    print(' ... done.')
